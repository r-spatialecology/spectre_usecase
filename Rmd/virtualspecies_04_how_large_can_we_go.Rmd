---
title: "How large can we go"
author: "Jan Salecker"
date: "8/31/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## How large can we go

This analysis is intended to assess the maximum landscape size we are comfortable with.
We will vary the landscape size only and see how the energy develops.
We start with the easiest case: strong correlation between species and low gamma.

## Test 01

```{r packages}
library(spectre)
library(tidyverse)
library(future)
library(virtualspecies)
library(furrr)
library(clustermq)
library(landscapetools)

## Source spectre analysis functions (R folder)
sapply(list.files("../R", full.names = TRUE), source)
set.seed(3562347)
exec <- "HPC" # "HPC"
dir.hpc <- file.path("/home/uni08/jsaleck/spectre")
dir.cloud <- file.path("/home/jan/netlogo_jan/spectre_usecase")

## Constant parameters:
replicates <- 5
max_runs <- 50000
energy_threshold <- 0
beta <- 0.75

## Variable parameters:
landscape_size <- c(10,25,50,75,100)
corr_within <- 1
corr_among <- 0.1
gamma <- 100
random_seeds <- round(runif(replicates) * 100000)

## Generate parameter matrix:
parameters <- expand.grid(landscape_size = landscape_size, 
                          corr_within = corr_within,
                          gamma = gamma, 
                          beta = beta,
                          corr_among = corr_among,
                          random_seeds = random_seeds)

if (exec == "local") {
  plan(multisession)
  results <- furrr::future_map_dfr(seq(nrow(parameters)), function(x) {
    virtualspecies_simfun(siminputrow = x, 
                          parameters = parameters,
                          max_runs = max_runs,
                          energy_threshold = energy_threshold,
                          writeRDS = FALSE,
                          outdir = dir.cloud)
  })
}
if (exec == "HPC") {
  
  maxjobs.hpc <- 2000
  njobs <- min(nrow(parameters), maxjobs.hpc)
  jobIDs <- seq(nrow(parameters))
  
  results <- clustermq::Q(fun = virtualspecies_simfun, 
                          siminputrow = jobIDs,
                          const = list(parameters = parameters,
                                       max_runs = max_runs,
                                       energy_threshold = energy_threshold,
                                       writeRDS = TRUE,
                                       outdir = dir.hpc),
                          export = list(), 
                          seed = 42, 
                          n_jobs = njobs, 
                          template = list(job_name = "spectre", # define jobname
                                          log_file = "spectre.log", # define logfile name
                                          queue = "medium",  # define HPC queue
                                          service = "normal", # define HPC service
                                          walltime = "24:00:00", # define walltime
                                          mem_cpu = "4000")) # define memory per cpu   
  results <- dplyr::bind_rows(results)
}

#### Restore results from rds files if neccessary:
#results <- purrr::map_dfr(list.files(dir.cloud, pattern = "rds", full.names = TRUE), function(x) {
#  res.x <- readRDS(x)
#  return(res.x)
#})

#### STORE RESULTS:
saveRDS(results, file=file.path(dir.cloud, "data", "virtualspecies_04_howlargecanwego_data.rds"))

```

## Analysis


#### Energy over time


#### Minimum energy

standardized regression coefficient:
summary(lm(scale(energy) ~ scale(landscape_size) * scale(gamma) * scale(corr_within) * scale(corr_among), data=energy_min))

#### Realtive commonness error



